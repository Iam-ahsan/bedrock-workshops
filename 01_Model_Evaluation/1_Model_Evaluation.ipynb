{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b47ece3",
   "metadata": {},
   "source": [
    "# Overview \n",
    "In this workshop, we will show you some common patterns for model evaluation. We will be evaluating a LLMs ability to do Q&A by:\n",
    "1. Creating a validation test set\n",
    "1. Define metrics that make sense for our usecase both qualitative and quantitative\n",
    "1. Create a quality assurance rubric that an large language model (LLM) can use to grade qualitatively\n",
    "1. Run our test Suite\n",
    "\n",
    "\n",
    "## Building the Evaluation Framework\n",
    "To build a strong testing framework, we need to start with a set of (mostly) human curated question / answer pairs. These will be our gold standard of what the model should be doing; the quality of these questions will drive the quality of our entire workload. Ideally, we need to manually create at least 100, if not 100’s of questions, carefully crafting correct answers to each. I strongly recommend that every generative AI project starts here because every minute spent building these test questions will pay back your investment 10 fold in time saved debugging and in quality of your final output. It’s also a great way to align your team on what the project is trying to accomplish.\n",
    "\n",
    "Resist the temptation to grab a premade list! Using your own questions from your use case will make a huge difference.\n",
    "\n",
    "\n",
    "When evaluating a prompt, we want **at least** 100 examples to run benchmarks on. One of the best ways to differentiate between a gen AI science project and a viable product is to count the number of automated tests. A handful of manual tests? Science Project. An automated system of hundreds of tests that runs every time you propose a change? Production ready.\n",
    "\n",
    "\n",
    "## Pre-Requisites\n",
    "\n",
    "Pre-requisites\n",
    "This notebook requires permissions to:\n",
    "\n",
    "create and delete Amazon IAM roles\n",
    "create, update and delete Amazon S3 buckets\n",
    "access Amazon Bedrock\n",
    "access to Amazon OpenSearch Serverless\n",
    "If running on SageMaker Studio, you should add the following managed policies to your role:\n",
    "\n",
    "1. AmazonS3FullAccess\n",
    "1. AmazonBedrockFullAccess\n",
    "1. Custom policy for Amazon OpenSearch Serverless such as:\n",
    "``` json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"aoss:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f81e6b",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required dependencies\n",
    "\n",
    "%pip install -U opensearch-py==2.3.1\n",
    "%pip install -U boto3==1.34.82\n",
    "%pip install -U langchain==0.1.13\n",
    "%pip install -U pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0937924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb513ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14d0f0",
   "metadata": {},
   "source": [
    "# Create Eval Dataset\n",
    "\n",
    "This part is a little tricky and time consuming. **For the purpose of this workshop, we went ahead and created a dataset**. We did this by using the prompt below to Bedrock (Claude3 Sonnet) and replacing the {pga_handbook} variable with chunks of the handbook. We did this a total of 5 times to create 100 examples. We also did some data massaging to convert the tsv to csv so it could be easily used. **Then we went through each example and manually validated the Q&A answers.**\n",
    "\n",
    "We recommend currating additional QnA examples manually to ensure a robust dataset. We also recommend you add to this eval dataset over time.\n",
    "\n",
    "## Prompt Used\n",
    "``` xml\n",
    "You are PromptWizard, an AI assistant designed to build synthetic evaluation Q&A samples based on the content of a PGA rule book. Your knowledge comes solely from from the content the Official PGA Tour Rulebook shown below\n",
    "\n",
    "<PGARuleBook>\n",
    "{pga_handbook}\n",
    "</PGARuleBook>\n",
    "\n",
    "<objective>\n",
    "Your objective is to generate 20 question and answer pairs using the pga rulebook that can be used to create an evaluation framework. \n",
    "</objective>\n",
    "\n",
    "\n",
    "\n",
    "<output_format>\n",
    "Question1\\tAnswer1\n",
    "Question2\\tAnswer2\n",
    "...\n",
    "Question20\\tAnswer20\n",
    "</output_format>\n",
    "\n",
    "<constraints>\n",
    "- All questions and answers must be directly relevant to the context of the and official rules for PGA Tour events\n",
    "- Make sure to write the question in a way that a user would ask the question. For example, \"What are you supposed to do when the ball lands in a spectators cup?\"\n",
    "- Answers should be factual, clear, and concise based on the playbook information\n",
    "- Ensure good variability in topics across the 20 Q&A pairs\n",
    "- No made up information - answers must come from the playbook context\n",
    "-  Separate the question and answer with a \\t character. We are generating a TSV.\n",
    "</constraints>\n",
    "```\n",
    "\n",
    "Next lets import our dataset into pandas and take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76467684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_csv('pga_tour_qna_eval_dataset.csv')\n",
    "\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69693f60",
   "metadata": {},
   "source": [
    "# Define Metrics for Benchmarking\n",
    "\n",
    "In this step, we'll be defining the metrics we care about in order to benchmark both different prompts and models. \n",
    "\n",
    "There are two main types of evaluation metrics. Qualitative and quantitative. Find descriptions of each type below. \n",
    "\n",
    "### Quantitative \n",
    "Quantitative metrics involve numerical measurements that can objectively compare different models. These typically include accuracy, perplexity, speed, and resource efficiency, among others. They provide a clear, standardized way to measure certain aspects of an LLM's performance, such as how well it predicts the next word in a sequence or how quickly it generates responses. \n",
    "\n",
    "For quantitative benchmarks we will create the following\n",
    "1. Latency\n",
    "\n",
    "### Qualitative\n",
    "On the other hand, qualitative metrics assess the more subjective aspects of LLM performance, including the coherence, relevancy, creativity of generated text, and adherence to ethical guidelines. These are often evaluated through human judgment via methods such as expert reviews or user studies, offering insights into the user experience and the contextual appropriateness of the model's outputs. While quantitative metrics can offer precise, measurable benchmarks, qualitative metrics are crucial for understanding the nuances and real-world effectiveness of LLMs. \n",
    "\n",
    "For qualitative evals we will want to consider\n",
    "1. Coherence of response\n",
    "2. Relevance of context passed to the model through RAG\n",
    "3. Accuracy of response\n",
    "4. Adherance to brand guidelines\n",
    "\n",
    "\n",
    "## How do we gather qualitative metrics?\n",
    "\n",
    "To gather qualitative metrics, we have two options. (1) Create a QA rubrik and give it to human evaluators or (2) Use that same rubrik and give it to an LLM to evaluate the responses. \n",
    "\n",
    "As a test suite gets larger, human evaluation becomes a bottleneck. Grading 500+ answers every time you make a change to a prompt is not scalable. Because of this, we'll opt to use an LLM to evaluate our responses. For poorly scoring responses, we can then manually check to see what's going on and fix the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe3a6c",
   "metadata": {},
   "source": [
    "## Lets create a grading prompt\n",
    "Below you'll find a prompt that takes in the question, model response, correct answer, and a rubric that you will create to evaluate models output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages.base import BaseMessage\n",
    "\n",
    "# We start by defining a \"grader prompt\" template.\n",
    "def build_grader_prompt(question, response, correct_answer, rubric) -> BaseMessage:\n",
    "    prompt = f\"\"\"You will be provided an answer that an assistant gave to a question, and a rubric that instructs you on what makes the answer correct or incorrect.\n",
    "    \n",
    "    Here is the question asked.\n",
    "    <question>{question}</question>\n",
    "    \n",
    "    Here is the response that the assistant gave to the question.\n",
    "    <assistant_response>{response}</assistant_response>\n",
    "    \n",
    "    Here is the correct answer.\n",
    "    <correct_answer>{correct_answer}</correct_answer>\n",
    "    \n",
    "    Here is the rubric on how to grade the assistant response.\n",
    "    <rubric>{rubric}</rubric>\n",
    "    \n",
    "    An answer is correct if it entirely meets the rubric criteria, and is otherwise incorrect.\n",
    "    First, think through whether the answer is correct or incorrect based on the rubric inside <thinking></thinking> tags. Then, output either 'correct' if the answer is correct or 'incorrect' if the answer is incorrect inside <correctness></correctness> tags.\n",
    "    \n",
    "    After thinking and providing a correctness score, use what's in the thinking tag to generate json that marks each of the following categories as True or False.\n",
    "    \n",
    "    categories\n",
    "    - accurateAnswer\n",
    "    - missingContext\n",
    "    - helpful\n",
    "    \n",
    "    Place the json in <categorized_eval></categorized_eval> tags.\n",
    "    \"\"\"\n",
    "\n",
    "    # First we will generate a prompt template using Langchain and the prompt above\n",
    "    chat_template: ChatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", prompt)\n",
    "    ])\n",
    "        \n",
    "    # Next, we will insert all the variables into into the prompt. \n",
    "    return chat_template.format_messages(\n",
    "        question=question,\n",
    "        response=response,\n",
    "        correct_answer=correct_answer,\n",
    "        rubric=rubric\n",
    "    )\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7012b",
   "metadata": {},
   "source": [
    "# !!! TODO\n",
    "\n",
    "As a next step, your task is to come up with a QA grading rubrik. This rubric should accurately describe what you're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your rubric here\n",
    "RUBRIC = '''\n",
    "TODO: Fill this out\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c3eec",
   "metadata": {},
   "source": [
    "# Call Q&A Bot To Get Responses\n",
    "In the following section, we will reuse the chat bot created from the previous workshop to get responses from it. We will then use your rubric to grade the models performance.\n",
    "\n",
    "\n",
    "To speed up validation, we will call our Q&A bot by submitting questions from our eval dataset into a thread pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e468ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SONNET_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "HAIKU_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# To flip between different models, you can change these global variable.\n",
    "MODEL_TO_USE = HAIKU_ID\n",
    "\n",
    "REGION = 'us-west-2'\n",
    "\n",
    "# TODO: Set this knowledge base value from the previous workshop \n",
    "KB_ID = '<FROM_PREVIOUS_WORKSHOP>'\n",
    "\n",
    "\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "\n",
    "\n",
    "def call_bedrock(request):\n",
    "    client = BedrockChat(\n",
    "        model_id=MODEL_TO_USE, \n",
    "        model_kwargs= {\"temperature\": 0.5, \"top_k\": 500}\n",
    "    )\n",
    "    \n",
    "    response = client.invoke(request)\n",
    "    return response\n",
    "\n",
    "def ask_bedrock_llm_with_knowledge_base(query: str) -> str:\n",
    "    start = time.time()\n",
    "    \n",
    "    model_arn: str = f'arn:aws:bedrock:{REGION}::foundation-model/{MODEL_TO_USE}'\n",
    "    \n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={ 'text': query },\n",
    "        retrieveAndGenerateConfiguration= {\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': KB_ID,\n",
    "                'modelArn': model_arn\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "\n",
    "    generated_text = response['output']['text']\n",
    "    \n",
    "    latency = time.time() - start\n",
    "    \n",
    "    return {\n",
    "        'modelResponse': generated_text,\n",
    "        'latency': latency\n",
    "    }\n",
    "\n",
    "# This is a bit funky. We're dumping all the requests into a thread pool\n",
    "# And storing the index for the order in which they were submitted. \n",
    "# Lastly, we're inserting them into the response array at their index to ensure order.\n",
    "def call_threaded(requests, function):\n",
    "    # Dictionary to map futures to their position\n",
    "    future_to_position = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        # Submit all requests and remember their order\n",
    "        for i, request in enumerate(requests):\n",
    "            future = executor.submit(function, request)\n",
    "            future_to_position[future] = i\n",
    "        \n",
    "        # Initialize an empty list to hold the responses\n",
    "        responses = [None] * len(requests)\n",
    "        \n",
    "        # As each future completes, assign its result to the correct position\n",
    "        for future in as_completed(future_to_position):\n",
    "            position = future_to_position[future]\n",
    "            try:\n",
    "                response = future.result()\n",
    "                responses[position] = response\n",
    "            except Exception as exc:\n",
    "                print(f\"Request at position {position} generated an exception: {exc}\")\n",
    "                responses[position] = None  # Or handle the exception as appropriate\n",
    "        \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e77a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "# Convert DataFrame to a list of dictionaries. This makes it easier to work with a thread pool.\n",
    "input_records: list[dict] = eval_df.to_dict('records')\n",
    "\n",
    "# Create prompts for all of our records.\n",
    "requests: list[str] = [r['question'] for r in input_records]\n",
    "\n",
    "# Call Bedrock threaded to speed up getting all our responses.\n",
    "responses: list[dict] = call_threaded(requests, ask_bedrock_llm_with_knowledge_base)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea05e837",
   "metadata": {},
   "source": [
    "# Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct grader prompt\n",
    "grader_prompts = []\n",
    "\n",
    "for i, r in enumerate(input_records):    \n",
    "    question = r['question']\n",
    "    response = responses[i]['modelResponse']\n",
    "    correct_answer = r['answer']\n",
    "    prompt: BaseMessage = build_grader_prompt(question, response, correct_answer, RUBRIC)\n",
    "    grader_prompts.append(prompt)\n",
    "\n",
    "    \n",
    "# Lets change the model we're using to Sonnet. It's generally good practice to use \n",
    "# a larger and more sophisticated model for grading.\n",
    "MODEL_TO_USE = SONNET_ID\n",
    "\n",
    "# Call Bedrock threaded to speed up getting all our responses.\n",
    "grades: list[AIMessage] = call_threaded(grader_prompts, call_bedrock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d768a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Strip out the correctness grade\n",
    "def extract_correctness(response):\n",
    "    # Regular expression to extract everything inside of the sumologquery tags\n",
    "    regex = r'<correctness>(.*?)</correctness>'\n",
    "    # Perform the regex search\n",
    "    matches = re.search(regex, response, re.DOTALL)\n",
    "    # Extract the matched content, if any\n",
    "    return matches.group(1).strip() if matches else None\n",
    "\n",
    "# Strip out the reasoning\n",
    "def extract_reasoning(response):\n",
    "    # Regular expression to extract everything inside of the sumologquery tags\n",
    "    regex = r'<thinking>(.*?)</thinking>'\n",
    "    # Perform the regex search\n",
    "    matches = re.search(regex, response, re.DOTALL)\n",
    "    # Extract the matched content, if any\n",
    "    return matches.group(1).strip() if matches else None\n",
    "    \n",
    "\n",
    "def format_grading_results(grade: str):\n",
    "    reasoning: str = extract_reasoning(grade)\n",
    "    correctness: str =  extract_correctness(grade)\n",
    "    \n",
    "    formatted_grading = {\n",
    "        'reasoning': reasoning,\n",
    "        'correctness': correctness\n",
    "    }\n",
    "        \n",
    "    return formatted_grading\n",
    "\n",
    "\n",
    "formatted_grades = [format_grading_results(g.content) for g in grades]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_records = []\n",
    "\n",
    "for i, r in enumerate(input_records):\n",
    "    question = r['question']\n",
    "    response = responses[i]['modelResponse']\n",
    "    latency = responses[i]['latency']\n",
    "    correct_answer = r['answer']\n",
    "    correctness = formatted_grades[i]['correctness']\n",
    "    reasoning = formatted_grades[i]['reasoning']\n",
    "    \n",
    "    eval_record = {\n",
    "        'question': question,\n",
    "        'correct_answer': correct_answer,\n",
    "        'model_response': response,\n",
    "        'correctness': correctness,\n",
    "        'reasoning': reasoning,\n",
    "        'latency': latency\n",
    "    }\n",
    "    \n",
    "    evaluated_records.append(eval_record)\n",
    "    \n",
    "    \n",
    "evaluated_df = pd.DataFrame(evaluated_records)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535efe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ab469",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Now that we have our new evaluation dataframe, lets do an analysis on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets check out the latency\n",
    "\n",
    "# Calculating statistics\n",
    "average_latency = evaluated_df['latency'].mean()\n",
    "median_latency = evaluated_df['latency'].median()\n",
    "min_latency = evaluated_df['latency'].min()\n",
    "max_latency = evaluated_df['latency'].max()\n",
    "\n",
    "print(\"Average Latency:\", average_latency)\n",
    "print(\"Median Latency:\", median_latency)\n",
    "print(\"Lowest Latency:\", min_latency)\n",
    "print(\"Top Latency:\", max_latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcadd65",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4266337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next lets see how many we got correct\n",
    "\n",
    "percentage_correct = evaluated_df['correctness'].value_counts(normalize=True)['correct'] * 100\n",
    "\n",
    "print(f\"Percentage correct: {percentage_correct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b77849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly we need to do some human evaluation. Lets sample a subsection of 10 incorrect responses\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Assuming you have a dataframe called 'df' with a column called 'result'\n",
    "incorrect_rows = evaluated_df[evaluated_df['correctness'] == 'incorrect'].sample(n=10)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Convert the dataframe to an HTML table\n",
    "table_html = incorrect_rows.to_html(index=False, classes='table table-striped')\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6abb38",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Now that you've run through this notebook. \n",
    "* Go back and play with the rubric. \n",
    "* You can also play with the temperature and other hyperparameters of the model to see how that has an effect on your score.\n",
    "* We used Haiku for the Q&A response. Set MODEL_TO_USE to SONNET_ID and rerun the test suite to see if that has an affect on the score.\n",
    "\n",
    "In the next section of the workshop, we'll discuss advanced RAG techniques and how that could have an affect on performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05244172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
